# -*- coding: utf-8 -*-
"""PWskills Basics Statistics Assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12Lnh_7qd-IJN8hiNb21FybsCM1-jJk4c

**Practical Questions**

---
"""

#1.How do you calculate the mean, median, and mode of a dataset.

"""1.Mean is simly the average of a dataset,i.e., it can be calculated by dividing sum of observations from the no. of observations in a dataset.
2.Median is the middlemost value of the dataset. It can be calculated with the help of following steps:
   Arrange the dataset in ascending order (smallest to largest).
   If there's an odd number of values, the median is the middle value.
   If there's an even number of values, the median is the average of the two middle values.
3. Mode: The mode is the value that appears most frequently in a dataset."""

import numpy as np
list = [2, 4, 4, 4, 5, 5, 7, 9]
print(np.mean(list))
print(np.median(list))
from collections import Counter
data = Counter(list)
data_list = dict(data)
print(max(data_list, key=data_list.get))

#2.Write a Python program to compute the variance and standard deviation of a dataset.
 list = [2, 4, 4, 4, 5, 5, 7, 9]
 print(np.var(list))
 print(np.std(list))

#3.Create a dataset and classify it into nominal, ordinal, interval, and ratio types.
 import pandas as pd
 customer_data=pd.DataFrame({"Customer ID":[101,102,103,104],#Nominal
                            "Gender":["Male","Female","Male","Female"],#Nominal
                            "Customer Rating":[3,4,4,5],#Ordinal
                            "Customer credit score":[-100,250,500,700],#Interval
                            "Age":[32,23,46,54]})#Ratio
 customer_data

#4.Implement sampling techniques like random sampling and stratified sampling.
import numpy as np
import pandas as pd


data_list = [2, 4, 4, 4, 5, 5, 7, 9]
sample_size = 3
random_sample = np.random.choice(data_list, size=sample_size, replace=False)
print("Random Sample:", random_sample)

data = pd.DataFrame({
    'values': [2, 4, 4, 4, 5, 5, 7, 9, 10, 11, 12, 13, 14, 15, 16],
    'strata_column': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C', 'A', 'A', 'B', 'B', 'C', 'C']
})

strata = data['strata_column'].unique()
stratified_sample = pd.DataFrame()

for stratum in strata:
    stratum_data = data[data['strata_column'] == stratum]
    sample_size = int(len(stratum_data) * 0.2)
    stratum_sample = stratum_data.sample(n=sample_size, replace=False)
    stratified_sample = pd.concat([stratified_sample, stratum_sample])
print("\nStratified Sample:")
print(stratified_sample)

#5.Write a Python function to calculate the range of a dataset.
data_1 = [2, 4, 4, 4, 5, 5, 7, 9]
data1_range = max(data_1) - min(data_1)
print(data1_range)

#6.Create a dataset and plot its histogram to visualize skewness.
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

Student_data=pd.DataFrame({"Exam Roll No.":[1001,1002,1003,1004,1005,1006,1007,1008,1009,1010],
                            "Marks":[80,80,92,80,92,80,92,43,40,50]})
skewness=Student_data['Marks'].skew()
print("Skewness:",skewness)

sns.histplot(Student_data['Marks'], kde=True)
plt.title("Distribution of Marks")
plt.xlabel("Marks")
plt.ylabel("Frequency")
plt.show()

#7.Calculate skewness and kurtosis of a dataset using Python libraries.
import scipy.stats as stats
import numpy as np

data = [2, 4, 4, 4, 5, 5, 7, 9]

skewness = stats.skew(data)
print("Skewness:", skewness)

kurtosis = stats.kurtosis(data)
print("Kurtosis:", kurtosis)

#8.Generate a dataset and demonstrate positive and negative skewness.

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Generate a dataset with positive skewness
data_positive_skew = np.random.exponential(scale=1, size=1000)

# Generate a dataset with negative skewness
data_negative_skew = np.random.beta(a=5, b=2, size=1000)

# Visualize the datasets using histograms
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
sns.histplot(data_positive_skew, kde=True)
plt.title("Positive Skewness")

plt.subplot(1, 2, 2)
sns.histplot(data_negative_skew, kde=True)
plt.title("Negative Skewness")

plt.show()

#9.Write a Python script to calculate covariance between two datasets.
import scipy.stats as stats
import numpy as np
Hrs_std=[2,3,2,4,2,3,3]
Marks_obt=[8,9,7,10,8,9,8]
Covariance=np.cov(Hrs_std,Marks_obt, bias=True)[0][1]
print("Covariance:",Covariance)

#10.Write a Python script to calculate the correlation coefficient between two datasets.
Hrs_std=[2,3,2,4,2,3,3]
Marks_obt=[8,9,7,10,8,9,8]
Correlation=np.corrcoef(Hrs_std,Marks_obt, bias=True)[0][1]
print("Correlation:",Correlation)

#11.Create a scatter plot to visualize the relationship between two variables.
Hrs_std=[2,3,2,4,2,3,3]
Marks_obt=[8,9,7,10,8,9,8]
import matplotlib.pyplot as plt
plt.scatter(x_data, y_data)
plt.xlabel("X-axis Label")
plt.ylabel("Y-axis Label")
plt.title("Scatter Plot of X vs. Y")
plt.show()

#12.Implement and compare simple random sampling and systematic sampling.

#random sampling
import numpy as np
students_rollno=list(range(1,101))
sample_size=10
random_sample=np.random.choice(students_rollno,size=sample_size,replace=False)
print("Simple Random Sample:",random_sample)

#systematic sampling
import numpy as np
students_rollno=list(range(1,101))
sample_size=5
k=len(students_rollno)//sample_interval
start=np.random.randint(1,k+1
                        )
systematic_sample = [students_rollno[i - 1] for i in range(start, len(students_rollno) + 1, k)]

print("Systematic Sample:", systematic_sample)

#13.Calculate the mean, median, and mode of grouped data.
import pandas as pd
import numpy as np
import scipy.stats as stats

data = [12, 15, 18, 21, 25, 28, 32, 35, 38, 42]
bins = [10, 20, 30, 40, 50]
grouped_data = pd.cut(data, bins)
print(grouped_data.value_counts())

numerical_data = [interval.mid for interval in grouped_data]

mean = np.mean(numerical_data)
print("Mean:", mean)
median = np.median(numerical_data)
print("Median:", median)
mode_result = stats.mode(numerical_data)
print("Mode:", mode_result.mode[0])

#14.Simulate data using Python and calculate its central tendency and dispersion.
 import numpy as np

data = np.random.normal(loc=50, scale=10, size=100)


mean = np.mean(data)
median = np.median(data)


variance = np.var(data)
std_dev = np.std(data)

print("Simulated Data:", data)
print("Mean:", mean)
print("Median:", median)
print("Variance:", variance)
print("Standard Deviation:", std_dev)

#15.Use NumPy or pandas to summarize a datasetâ€™s descriptive statistics.
import pandas as pd
data = [12, 15, 18, 21, 25, 28, 32, 35, 38, 42]
descriptive_stats = pd.Series(data).describe()
print(descriptive_stats)

#16.Plot a boxplot to understand the spread and identify outliers.
import matplotlib.pyplot as plt
import numpy as np

data = [12, 15, 18, 21, 25, 28, 32, 35, 38, 42, 100]
plt.figure(figsize=(8, 6))
plt.boxplot(data)
plt.title("Boxplot of Data")
plt.ylabel("Values")
plt.show()

#17. Calculate the interquartile range (IQR) of a dataset.
import numpy as np

data = [12, 15, 18, 21, 25, 28, 32, 35, 38, 42, 100]

q1 = np.percentile(data, 25)
q3 = np.percentile(data, 75)

iqr = q3 - q1

print("Q1:",q1)
print("Q3:",q3)
print("Interquartile Range (IQR):", iqr)

#18.Implement Z-score normalization and explain its significance.
import numpy as np
data = [12, 15, 18, 21, 25, 28, 32, 35, 38, 42]
def zscore_normalize(data):
  mean = np.mean(data)
  std_dev = np.std(data)
  normalized_data = [(x - mean) / std_dev for x in data]
  return normalized_data
normalized_data = zscore_normalize(data)

print("Original Data:", data)
print("Normalized Data:", normalized_data)

#19.Compare two datasets using their standard deviations.
import numpy as np

dataset1 = [10, 12, 14, 16, 18]
dataset2 = [2, 4, 6, 8, 10]


std_dev1 = np.std(dataset1)
std_dev2 = np.std(dataset2)

print("Standard Deviation of Dataset 1:", std_dev1)
print("Standard Deviation of Dataset 2:", std_dev2)

if std_dev1 > std_dev2:
    print("Dataset 1 has more spread/variability than Dataset 2.")
elif std_dev1 < std_dev2:
    print("Dataset 2 has more spread/variability than Dataset 1.")
else:
    print("Both datasets have similar spread/variability.")

#20.Write a Python program to visualize covariance using a heatmap.
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt


data = np.random.rand(10, 3)

covariance_matrix = np.cov(data, rowvar=False)

plt.figure(figsize=(8, 6))
sns.heatmap(covariance_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Covariance Heatmap")
plt.show()

#21.Use seaborn to create a correlation matrix for a dataset.
 import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

data = pd.DataFrame({
    'A': [1, 2, 3, 4, 5],
    'B': [2, 4, 5, 4, 6],
    'C': [1, 3, 4, 3, 5]
})


correlation_matrix = data.corr()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Matrix")
plt.show()

#22.Generate a dataset and implement both variance and standard deviation computations
import numpy as np


data = np.random.randint(10, 100, size=50)


variance = np.var(data)

std_dev = np.std(data)

print("Generated Dataset:", data)
print("Variance:", variance)
print("Standard Deviation:", std_dev)

#23.Visualize skewness and kurtosis using Python libraries like matplotlib or seaborn.
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats


data = np.random.normal(loc=0, scale=1, size=1000)

skewness = stats.skew(data)
kurtosis = stats.kurtosis(data)

plt.figure(figsize=(8, 6))
sns.histplot(data, kde=True)
plt.title(f"Distribution of Data\nSkewness: {skewness:.2f}, Kurtosis: {kurtosis:.2f}")
plt.xlabel("Values")

#24.Implement the Pearson and Spearman correlation coefficients for a dataset.
import numpy as np
import pandas as pd
from scipy.stats import pearsonr, spearmanr

data = pd.DataFrame({
    'X': [1, 2, 3, 4, 5],
    'Y': [2, 4, 1, 3, 5]
})
pearson_corr, _ = pearsonr(data['X'], data['Y'])
print(f"Pearson correlation: {pearson_corr:.2f}")

spearman_corr, _ = spearmanr(data['X'], data['Y'])
print(f"Spearman correlation: {spearman_corr:.2f}")





